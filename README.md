# ETTA-X: AI-Powered Test Automation & Impact Analysis Platform

<p align="center">
  <img src="frontend/static/assets/etta_x_logo.png" alt="ETTA-X Logo" width="120"/>
</p>

<p align="center">
  <strong>Intelligent Test Generation | Risk-Based Prioritization | Self-Healing Tests</strong>
</p>

<p align="center">
  <a href="#features">Features</a> â€¢
  <a href="#quick-start">Quick Start</a> â€¢
  <a href="#architecture">Architecture</a> â€¢
  <a href="#api-reference">API Reference</a> â€¢
  <a href="#credits">Credits</a>
</p>

---

## ğŸ“‹ Overview

**ETTA-X** (Enterprise Test & Transformation Accelerator - eXtended) is an AI-powered platform that automatically generates, prioritizes, and manages test cases based on code changes. It uses machine learning for impact analysis and local LLM models for intelligent test generation.

> âš ï¸ **Infrastructure Note**: Due to infrastructure limitations, the demo executable uses **Google Gemini API** for test generation. The actual local prototype uses **CodeLlama 7B** running via Ollama for completely offline, privacy-preserving test generation.

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| **ğŸ” Impact Analysis** | Automatically analyzes code changes and calculates risk scores |
| **ğŸ¤– AI Test Generation** | Generates test cases using CodeLlama (local) or Gemini (demo) |
| **ğŸ“Š Risk-Based Prioritization** | ML model prioritizes tests based on change risk and code criticality |
| **ğŸ”— GitHub Integration** | Webhook-based pipeline triggers on every push |
| **ğŸ“ˆ Dashboard Analytics** | Real-time metrics on repositories, analyses, and test coverage |
| **ğŸŒ™ Dark/Light Theme** | Full theme support for comfortable viewing |

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+**
- **Node.js 18+**
- **Git**
- **Ollama** (for local LLM - optional for demo)
- **ngrok** (for GitHub webhook tunneling)

### Step-by-Step Setup

#### Step 1: Clone the Repository

```bash
git clone https://github.com/EttaX-Developers/ETTA-X.git
cd ETTA-X
```

#### Step 2: Install Python Dependencies

```bash
# Create virtual environment
python -m venv .venv

# Activate (Windows)
.venv\Scripts\Activate.ps1

# Activate (Linux/Mac)
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
pip install -r LLM/requirements.txt
```

#### Step 3: Install Ollama & CodeLlama (Local LLM)

```bash
# Download Ollama from https://ollama.ai
# Then pull CodeLlama model:
ollama pull codellama:7b-instruct

# Start Ollama server
ollama serve
```

#### Step 4: Start the Backend Server

```bash
# From project root
python -m uvicorn backend.app.app:app --host 0.0.0.0 --port 8000 --reload
```

#### Step 5: Setup ngrok for GitHub Webhooks

```bash
# Start ngrok tunnel
ngrok http 8000

# Copy the HTTPS URL (e.g., https://xxxx.ngrok.io)
```

#### Step 6: Configure GitHub Webhook

1. Go to your GitHub repository â†’ **Settings** â†’ **Webhooks**
2. Click **Add webhook**
3. Set **Payload URL**: `https://your-ngrok-url.ngrok.io/api/github/webhook`
4. Set **Content type**: `application/json`
5. Select **Just the push event**
6. Click **Add webhook**

#### Step 7: Launch the Desktop App

**Option A: Using the batch file**
```bash
# Double-click ETTAX_Beta.bat
# OR run:
.\ETTAX_Beta.bat
```

**Option B: Manual launch**
```bash
cd electron
npm install
npm run setup
```

#### Step 8: Login with GitHub

1. Click "Sign in with GitHub" in the app
2. Authorize the ETTA-X application
3. Your repositories will automatically appear

---

## ğŸ“– Using the Features

### 1. Dashboard

The main dashboard displays:
- Total repositories connected
- Number of analyses performed
- High-risk changes detected
- Tests suggested by the AI

### 2. Repositories

- View all connected GitHub repositories
- See last commit and analysis status
- Click to view detailed repository info

### 3. Impact Analysis

1. Navigate to **Impact Analysis** from the sidebar
2. View all webhook events from your repositories
3. Each event shows:
   - **Risk Score** (0-100): Higher = more critical
   - **Files Changed**: Number of modified files
   - **Change Types**: Type of modifications detected
4. Click on an event to see detailed analysis

### 4. Test Runs

1. Navigate to **Test Runs** from the sidebar
2. View all AI-generated tests
3. Filter by:
   - **Priority**: High, Medium, Low
   - **Status**: Pending, Passed, Failed
4. Click a test to see:
   - Test name and description
   - HTTP method and endpoint
   - Expected status code
   - Priority score and category

### 5. Triggering the Pipeline

Simply push code to your connected repository:

```bash
git add .
git commit -m "Your changes"
git push origin main
```

The pipeline automatically:
1. âœ… Receives webhook from GitHub
2. âœ… Analyzes code diff
3. âœ… Calculates impact/risk score
4. âœ… Generates tests via AI (if risk > threshold)
5. âœ… Prioritizes tests using ML model
6. âœ… Updates the UI in real-time

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ETTA-X Architecture                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  GitHub  â”‚â”€â”€â”€â”€â–¶â”‚  Webhook â”‚â”€â”€â”€â”€â–¶â”‚   Backend (FastAPI)  â”‚    â”‚
â”‚  â”‚   Push   â”‚     â”‚ Receiver â”‚     â”‚                      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚                                     â”‚  â”‚ Diff Analyzer  â”‚  â”‚    â”‚
â”‚                                     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚                                     â”‚          â”‚           â”‚    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚ Electron â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  â”‚Impact Analyzer â”‚  â”‚    â”‚
â”‚  â”‚   App    â”‚     REST API         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚          â”‚           â”‚    â”‚
â”‚                                     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚                                     â”‚  â”‚  LLM (Ollama)  â”‚  â”‚    â”‚
â”‚                                     â”‚  â”‚   CodeLlama    â”‚  â”‚    â”‚
â”‚                                     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚                                     â”‚          â”‚           â”‚    â”‚
â”‚                                     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚                                     â”‚  â”‚ ML Prioritizer â”‚  â”‚    â”‚
â”‚                                     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ API Reference

### Webhook Endpoint
```
POST /api/github/webhook
```
Receives GitHub push events and triggers the analysis pipeline.

### Impact Analysis
```
GET /api/analysis/events
```
Returns all analyzed webhook events with risk scores.

### Generated Tests
```
GET /api/tests/generated
```
Returns all AI-generated tests from the pipeline.

### Test Pipeline Status
```
GET /api/tests/status
```
Returns LLM status and pipeline health.

---

## ğŸ“¦ Project Structure

```
ETTA-X/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ diff_analyzer.py      # Git diff analysis
â”‚   â”‚   â”œâ”€â”€ git_repo.py           # Repository management
â”‚   â”‚   â””â”€â”€ test_pipeline.py      # Test generation pipeline
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ app.py                # FastAPI application
â”‚   â”‚   â””â”€â”€ database.py           # SQLite database
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ etta_x.db             # Application database
â”‚   â””â”€â”€ model/
â”‚       â””â”€â”€ LLM/                  # LLM integration
â”œâ”€â”€ electron/
â”‚   â”œâ”€â”€ main.js                   # Electron main process
â”‚   â””â”€â”€ preload.js                # Preload scripts
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ pages/                    # HTML pages
â”‚   â””â”€â”€ static/                   # CSS, JS, assets
â”œâ”€â”€ LLM/
â”‚   â””â”€â”€ llm_model.py              # Ollama/LLM wrapper
â”œâ”€â”€ Z_Data_set/
â”‚   â”œâ”€â”€ train_model.py            # ML model training
â”‚   â””â”€â”€ etta_x_dataset.csv        # Training dataset
â”œâ”€â”€ process_pending.py            # Pipeline processor
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ ETTAX_Beta.bat               # Quick start script
â””â”€â”€ README.md                     # This file
```

---

## ğŸ™ Credits & Acknowledgments

### External APIs

| API | Usage | Link |
|-----|-------|------|
| **Ollama** | Local LLM inference server | [ollama.ai](https://ollama.ai) |
| **Google Gemini** | Demo LLM (API fallback) | [ai.google.dev](https://ai.google.dev) |
| **GitHub API** | OAuth & webhook integration | [docs.github.com](https://docs.github.com) |

### Machine Learning Models

| Model | Purpose | Source |
|-------|---------|--------|
| **CodeLlama 7B Instruct** | Test case generation | [Meta AI](https://ai.meta.com/llama/) |
| **XGBoost** | Test prioritization | [xgboost.readthedocs.io](https://xgboost.readthedocs.io) |
| **Scikit-learn** | Feature engineering | [scikit-learn.org](https://scikit-learn.org) |

### Libraries & Frameworks

| Library | Version | Purpose |
|---------|---------|---------|
| FastAPI | 0.100+ | Backend REST API |
| Electron | 28+ | Desktop application |
| SQLAlchemy | 2.0+ | Database ORM |
| GitPython | 3.1+ | Git operations |
| httpx | 0.25+ | Async HTTP client |
| Pandas | 2.0+ | Data processing |
| NumPy | 1.24+ | Numerical computing |

### Datasets

- Custom synthetic dataset generated for test prioritization training
- Located in `Z_Data_set/etta_x_enhanced.csv`

---

## âš™ï¸ Configuration

### Environment Variables (Optional)

```bash
# For demo mode with Gemini
GEMINI_API_KEY=your_api_key_here

# GitHub OAuth (already configured)
GITHUB_CLIENT_ID=your_client_id
GITHUB_CLIENT_SECRET=your_client_secret
```

### LLM Configuration

Edit `LLM/llm_model.py` to switch between:
- **Ollama (Local)**: Default, privacy-preserving
- **Gemini (Cloud)**: Fallback for demo

---

## ğŸ”’ Privacy & Security

- **Local Processing**: CodeLlama runs entirely on your machine
- **No Code Upload**: Your code never leaves your infrastructure
- **Secure OAuth**: GitHub OAuth 2.0 for authentication
- **Encrypted Storage**: SQLite with local file permissions

---

## ğŸ› Troubleshooting

### Ollama not responding
```bash
# Check if Ollama is running
ollama list

# Restart Ollama
ollama serve
```

### Webhook not triggering
1. Check ngrok is running and URL is correct
2. Verify webhook settings in GitHub
3. Check backend logs for errors

### Tests not generating
- Ensure Ollama has the `codellama:7b-instruct` model
- Check `process_pending.py` output for errors
- Verify webhook events are being received

---

## ğŸ“„ License

This project is developed for the ETTA-X Hackathon 2026.

---

## ğŸ‘¥ Team

**EttaX-Developers**

---

<p align="center">
  Made with â¤ï¸ for intelligent test automation
</p>
