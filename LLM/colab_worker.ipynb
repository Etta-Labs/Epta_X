{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ LLM GPU Worker (Colab)\n",
    "\n",
    "This notebook runs llama.cpp server on Colab's T4 GPU and exposes it via Cloudflare Tunnel.\n",
    "\n",
    "## Prerequisites\n",
    "1. **Google Drive** with:\n",
    "   - `llama.cpp/` folder containing compiled `llama-server`\n",
    "   - Your GGUF model file\n",
    "2. **Cloudflare Account** with a tunnel token\n",
    "3. **Oracle VM** running the FastAPI gateway\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "Edit these values before running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Edit these values!\n",
    "# =============================================================================\n",
    "\n",
    "# Path to your GGUF model in Google Drive\n",
    "MODEL_PATH = \"/content/drive/MyDrive/llm/models/your-model.gguf\"\n",
    "\n",
    "# Path to llama.cpp folder in Google Drive (should contain llama-server binary)\n",
    "LLAMA_CPP_PATH = \"/content/drive/MyDrive/llm/llama.cpp\"\n",
    "\n",
    "# Your Cloudflare Tunnel token (get from Cloudflare Zero Trust dashboard)\n",
    "# This should be for a tunnel configured to point to localhost:8000\n",
    "CLOUDFLARE_TUNNEL_TOKEN = \"your-tunnel-token-here\"\n",
    "\n",
    "# Your Oracle VM Gateway URL (where to register this worker)\n",
    "GATEWAY_URL = \"https://your-gateway-domain.com\"\n",
    "\n",
    "# llama-server settings\n",
    "LLAMA_PORT = 8000\n",
    "CONTEXT_SIZE = 4096  # Adjust based on your model and GPU memory\n",
    "GPU_LAYERS = 99  # Number of layers to offload to GPU (-1 for all)\n",
    "THREADS = 4\n",
    "\n",
    "print(\"‚úÖ Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify paths exist\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model_size = os.path.getsize(MODEL_PATH) / (1024**3)\n",
    "    print(f\"‚úÖ Model found: {MODEL_PATH} ({model_size:.2f} GB)\")\n",
    "else:\n",
    "    print(f\"‚ùå Model NOT found at: {MODEL_PATH}\")\n",
    "    print(\"Please update MODEL_PATH in the configuration cell!\")\n",
    "\n",
    "if os.path.exists(LLAMA_CPP_PATH):\n",
    "    print(f\"‚úÖ llama.cpp folder found: {LLAMA_CPP_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è llama.cpp not found. Will compile from source...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 2: Setup llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "LLAMA_SERVER_PATH = os.path.join(LLAMA_CPP_PATH, \"build/bin/llama-server\")\n",
    "\n",
    "# Check if llama-server already exists in Drive\n",
    "if os.path.exists(LLAMA_SERVER_PATH):\n",
    "    print(f\"‚úÖ llama-server already compiled at: {LLAMA_SERVER_PATH}\")\n",
    "    # Make it executable\n",
    "    os.chmod(LLAMA_SERVER_PATH, 0o755)\n",
    "else:\n",
    "    print(\"üî® Compiling llama.cpp from source...\")\n",
    "    print(\"This will take a few minutes but only needs to be done once.\")\n",
    "    \n",
    "    # Clone if not exists\n",
    "    if not os.path.exists(LLAMA_CPP_PATH):\n",
    "        !git clone https://github.com/ggerganov/llama.cpp {LLAMA_CPP_PATH}\n",
    "    \n",
    "    # Install CUDA toolkit\n",
    "    !apt-get update && apt-get install -y cmake build-essential\n",
    "    \n",
    "    # Build with CUDA support\n",
    "    os.chdir(LLAMA_CPP_PATH)\n",
    "    !cmake -B build -DGGML_CUDA=ON\n",
    "    !cmake --build build --config Release -j$(nproc)\n",
    "    \n",
    "    if os.path.exists(LLAMA_SERVER_PATH):\n",
    "        print(\"‚úÖ llama.cpp compiled successfully!\")\n",
    "    else:\n",
    "        print(\"‚ùå Compilation failed. Check the output above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Step 3: Install Cloudflare Tunnel Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install cloudflared\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
    "!dpkg -i cloudflared-linux-amd64.deb\n",
    "!rm cloudflared-linux-amd64.deb\n",
    "\n",
    "# Verify installation\n",
    "!cloudflared --version\n",
    "print(\"\\n‚úÖ Cloudflared installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: Start LLM Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import threading\n",
    "\n",
    "# Build the command\n",
    "cmd = [\n",
    "    LLAMA_SERVER_PATH,\n",
    "    \"--model\", MODEL_PATH,\n",
    "    \"--host\", \"0.0.0.0\",\n",
    "    \"--port\", str(LLAMA_PORT),\n",
    "    \"--ctx-size\", str(CONTEXT_SIZE),\n",
    "    \"--n-gpu-layers\", str(GPU_LAYERS),\n",
    "    \"--threads\", str(THREADS),\n",
    "]\n",
    "\n",
    "print(f\"üöÄ Starting llama-server...\")\n",
    "print(f\"   Command: {' '.join(cmd)}\")\n",
    "\n",
    "# Start the server in background\n",
    "llama_process = subprocess.Popen(\n",
    "    cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Function to print server output\n",
    "def print_output():\n",
    "    for line in llama_process.stdout:\n",
    "        print(line, end='')\n",
    "\n",
    "# Start output thread\n",
    "output_thread = threading.Thread(target=print_output, daemon=True)\n",
    "output_thread.start()\n",
    "\n",
    "# Wait for server to start\n",
    "print(\"\\n‚è≥ Waiting for server to load model...\")\n",
    "for i in range(120):  # 2 minute timeout\n",
    "    try:\n",
    "        response = requests.get(f\"http://localhost:{LLAMA_PORT}/health\", timeout=2)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"\\n‚úÖ llama-server is ready on port {LLAMA_PORT}!\")\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(1)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"   Still loading... ({i}s)\")\n",
    "else:\n",
    "    print(\"‚ùå Timeout waiting for server to start\")\n",
    "    print(\"Check the output above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Step 5: Start Cloudflare Tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "import re\n",
    "\n",
    "# Start cloudflared tunnel\n",
    "print(\"üîó Starting Cloudflare Tunnel...\")\n",
    "\n",
    "tunnel_process = subprocess.Popen(\n",
    "    [\"cloudflared\", \"tunnel\", \"run\", \"--token\", CLOUDFLARE_TUNNEL_TOKEN],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Capture tunnel output\n",
    "tunnel_url = None\n",
    "\n",
    "def monitor_tunnel():\n",
    "    global tunnel_url\n",
    "    for line in tunnel_process.stdout:\n",
    "        print(line, end='')\n",
    "        # Look for connection established message\n",
    "        if \"Registered tunnel connection\" in line or \"Connection registered\" in line:\n",
    "            tunnel_url = \"connected\"\n",
    "\n",
    "tunnel_thread = threading.Thread(target=monitor_tunnel, daemon=True)\n",
    "tunnel_thread.start()\n",
    "\n",
    "# Wait for tunnel to connect\n",
    "print(\"‚è≥ Waiting for tunnel connection...\")\n",
    "for i in range(30):\n",
    "    if tunnel_url:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "if tunnel_url:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ CLOUDFLARE TUNNEL CONNECTED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nYour llama-server is now accessible via your Cloudflare domain.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Tunnel may still be connecting. Check the output above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì° Step 6: Register with Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# The worker URL is your Cloudflare tunnel URL for the worker\n",
    "# You need to configure this in Cloudflare Zero Trust dashboard\n",
    "# For example: https://llm-worker.your-domain.com\n",
    "WORKER_PUBLIC_URL = \"https://llm-worker.your-domain.com\"  # UPDATE THIS!\n",
    "\n",
    "# Register with the gateway\n",
    "print(f\"üì° Registering with gateway at {GATEWAY_URL}...\")\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        f\"{GATEWAY_URL}/register-worker\",\n",
    "        json={\n",
    "            \"worker_url\": WORKER_PUBLIC_URL,\n",
    "            \"model_name\": os.path.basename(MODEL_PATH)\n",
    "        },\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"\\n‚úÖ {result['message']}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Registration returned: {response.status_code}\")\n",
    "        print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Failed to register: {e}\")\n",
    "    print(\"Make sure your gateway is running and accessible.\")\n",
    "\n",
    "# Print ready signal\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ READY - Worker is now serving requests!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Worker URL: {WORKER_PUBLIC_URL}\")\n",
    "print(f\"üè† Gateway URL: {GATEWAY_URL}\")\n",
    "print(f\"\\n‚ö†Ô∏è Keep this notebook running to serve requests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Step 7: Test Local Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Test completion locally\n",
    "print(\"üß™ Testing local inference...\\n\")\n",
    "\n",
    "test_prompt = \"\"\"Write a simple Python function that adds two numbers:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "\n",
    "response = requests.post(\n",
    "    f\"http://localhost:{LLAMA_PORT}/completion\",\n",
    "    json={\n",
    "        \"prompt\": test_prompt,\n",
    "        \"n_predict\": 100,\n",
    "        \"temperature\": 0.7,\n",
    "        \"stop\": [\"```\"]\n",
    "    },\n",
    "    timeout=60\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"‚úÖ Response received!\\n\")\n",
    "    print(f\"Generated code:\\n```python\\n{result['content']}```\\n\")\n",
    "    print(f\"Tokens generated: {result.get('tokens_predicted', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõë Cleanup (Run before disconnecting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Unregister from gateway\n",
    "print(\"üõë Cleaning up...\")\n",
    "\n",
    "try:\n",
    "    requests.post(f\"{GATEWAY_URL}/unregister-worker\", timeout=5)\n",
    "    print(\"‚úÖ Unregistered from gateway\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Could not unregister from gateway\")\n",
    "\n",
    "# Stop processes\n",
    "try:\n",
    "    tunnel_process.terminate()\n",
    "    print(\"‚úÖ Cloudflare tunnel stopped\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    llama_process.terminate()\n",
    "    print(\"‚úÖ llama-server stopped\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\nüëã Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
